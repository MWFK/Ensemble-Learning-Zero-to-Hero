{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1- Randomized Search vs Grid Search.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPb69vnemzeXUz60q6cqNut",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MWFK/Ensemble-Learning-Zero-to-Hero/blob/main/1_Randomized_Search_vs_Grid_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQh9eQpg_glb"
      },
      "source": [
        "# Comparing randomized search and grid search for hyperparameter estimation\n",
        "\n",
        "\n",
        "Compare randomized search and grid search for optimizing hyperparameters of a\n",
        "linear SVM with SGD training.\n",
        "All parameters that influence the learning are searched simultaneously\n",
        "(except for the number of estimators, which poses a time / quality tradeoff).\n",
        "\n",
        "The randomized search and the grid search explore exactly the same space of\n",
        "parameters. The result in parameter settings is quite similar, while the run\n",
        "time for randomized search is drastically lower.\n",
        "\n",
        "The performance is may slightly worse for the randomized search, and is likely\n",
        "due to a noise effect and would not carry over to a held-out test set.\n",
        "\n",
        "Note that in practice, one would not search over this many different parameters\n",
        "simultaneously using grid search, but pick only the ones deemed most important.\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZHwk8d6_PKv"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8gZ78_ICbP4"
      },
      "source": [
        "# GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sg96UTi_ufS",
        "outputId": "7c3a5683-cddd-4849-c2c5-d5c9158f72ba"
      },
      "source": [
        "#print(__doc__)\n",
        "import numpy as np\n",
        "from time import time\n",
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# get some data\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "# build a classifier\n",
        "clf = SGDClassifier(loss='hinge', penalty='elasticnet',fit_intercept=True)\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
        "                  .format(results['mean_test_score'][candidate],\n",
        "                          results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "\n",
        "# use a full grid over all parameters\n",
        "param_grid = {'average': [True, False],\n",
        "              'l1_ratio': np.linspace(0, 1, num=10),\n",
        "              'alpha': np.power(10, np.arange(-4, 1, dtype=float))}\n",
        "\n",
        "# run grid search\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
        "start = time()\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
        "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
        "report(grid_search.cv_results_)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GridSearchCV took 135.60 seconds for 100 candidate parameter settings.\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.929 (std: 0.026)\n",
            "Parameters: {'alpha': 0.0001, 'average': True, 'l1_ratio': 0.0}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.928 (std: 0.029)\n",
            "Parameters: {'alpha': 0.0001, 'average': True, 'l1_ratio': 0.4444444444444444}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.926 (std: 0.035)\n",
            "Parameters: {'alpha': 0.01, 'average': False, 'l1_ratio': 0.3333333333333333}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmhJVKRFCrcX"
      },
      "source": [
        "# RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT-Fe48ICt4g",
        "outputId": "71afd487-8ed3-4778-9878-04c4a6d7f046",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "import scipy.stats as stats\n",
        "#from sklearn.utils.fixes import loguniform # import bug\n",
        "#from loguniform import LogUniform # rvs() random_state error # this lib mimics the scipy.stats and it's implemnetd in scypi.stats.reciprocal\n",
        "from scipy.stats import reciprocal # this replace the last tow previous libs\n",
        "# loguniform/reciprocal : the loguniform is an approximate distribution, usually used in expert estimates to desccribe a variable that might take w wide range\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# get some data\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "# build a classifier\n",
        "clf = SGDClassifier(loss='hinge', penalty='elasticnet',\n",
        "                    fit_intercept=True)\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
        "                  .format(results['mean_test_score'][candidate],\n",
        "                          results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {'average': [True, False],\n",
        "              'l1_ratio': stats.uniform(0, 1),\n",
        "              'alpha': reciprocal(1e-4, 1e0)}\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search)\n",
        "\n",
        "start = time()\n",
        "random_search.fit(X, y)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n",
            "RandomizedSearchCV took 24.43 seconds for 20 candidates parameter settings.\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.927 (std: 0.034)\n",
            "Parameters: {'alpha': 0.00018876332719840208, 'average': True, 'l1_ratio': 0.008115873004421403}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.922 (std: 0.030)\n",
            "Parameters: {'alpha': 0.0011589403951646953, 'average': True, 'l1_ratio': 0.30034396045842104}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.917 (std: 0.026)\n",
            "Parameters: {'alpha': 0.36950301734634594, 'average': False, 'l1_ratio': 0.14418787298583324}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}